import base64
import asyncio
from openai import AsyncOpenAI
import aiofiles
import json

from tqdm import tqdm
import pandas as pd

from pathlib import Path

import os
from datetime import datetime
from utils import ru_sense_with_eng_meanings_in_brackets, ru_sense_with_ru_meanings_in_brackets, only_eng_meaning

"""
In this script, a set of images with textual instructions is processed,
where meanings are provided in one prompt.
"""


API_KEY = "EMPTY"
BASE_URL = "http://0.0.0.0:20000/v1"
MODEL_ID = "Qwen/Qwen2.5-VL-72B-Instruct"
N_PER_REQUEST = 10
MAX_INFLIGHT = 40

TEMPERATURE = 0.2
TOPP = 0.9
MAX_TOKENS = 2048 # must be lower then model_len

MEANING_STRATEGIES = {
    'prompt1': ru_sense_with_eng_meanings_in_brackets,
    'prompt2': ru_sense_with_ru_meanings_in_brackets,
    'prompt3': only_eng_meaning,
}


def png_to_data_url(path: str) -> str:
    """Read a PNG and return a data-URL the OpenAI spec understands."""
    b64 = base64.b64encode(Path(path).read_bytes()).decode("ascii")
    return f"data:image/png;base64,{b64}"


def postprocessing_wrapper(resp: list):
    """Postprocessing of responses list.

    Args:
        resp (list): List of responses from the model.

    Returns:
        list[str]: List of verdicts from the model.
    """
    texts = [ch.message.content for ch in resp.choices]
    return texts


async def one_call(client, text_instruction: str, path_to_img: str, seed: int):
    """Processing a single image.

    Args:
        client (AsyncOpenAI?): OpenAI client.
        text_instruction (str): Instruction for VLLM.
        path_to_img (str): Path to the image.
        seed (int): Generation seed. Does not guarantee or provide determinism
        under --max-num-seqs > 1 conditions (continuous batching)

    Returns:
        list[str]: List of predictions for the image + text query.
    """
    responses = await client.chat.completions.create(
        model=MODEL_ID,
        n=N_PER_REQUEST,

        temperature=TEMPERATURE,
        top_p=TOPP,
        max_tokens=MAX_TOKENS,
        seed=seed,

        messages=[{
            "role": "user",
            "content": [
                {"type": "text",
                 "text": text_instruction},
                {"type": "image_url",
                 "image_url": {"url": png_to_data_url(path_to_img)}}
            ],
        }],
    )

    return postprocessing_wrapper(responses)


def extracting_verdict(answer: str):
    """Extract the main verdict from the VLLM response (located at the end of generations).

    Args:
        answer (str): VLLM response.

    Returns:
        str: Model verdict at the end of the prompt. Sometimes not parsed,
        and N/A, N/A2 is returned (needs to be caught and debugged).
    """
    index = max(
        answer.rfind("DUPLICATE: "),
        answer.rfind("DUPLICATE:** "),
        answer.rfind("DUPLICATE**: "),
    )
    if index == -1:
        return "N/A"

    subanswer = answer[index + 11:]  # len(DUPLICATE: )

    if 'TRUE' in subanswer and 'FALSE' not in subanswer:
        return "TRUE"

    if 'FALSE' in subanswer and 'TRUE' not in subanswer:
        return "FALSE"

    return "N/A2"


# templates are not varied! It's one template
template = lambda homonymword, str_senses: f"""There is a problem, which is called as Homonym Duplication.
It is when in the image, which was generated by text2image model there are several senses of the input prompt.
This image is generated by a neural network for a multi-senses short prompt: "{homonymword}".
This prompt can take several values (in Russian):
{str_senses}
Possibly there are other values.
The meaning of the homonym may be implicit in the picture.
For each meaning, reason step-by-step and mark the presence in the picture and provide the answer in the following template:
sense_1: [Explicit|Implicit|Absent], justification
sense_2: [Explicit|Implicit|Absent], justification
...
[Reasoning, summarization].
DUPLICATE: [TRUE|FALSE].
An image may be connected to a meaning implicitly, through association or related meaning.
Therefore, be very attentive and carefully study the picture for the presence of the listed meanings, even if they are presented implicitly.

Note:
* Implicit covers metaphors, hints, visual puns.
* Base judgments only on what is visible (including any text shown).
* Base your final words in answer as DUPLICATE: TRUE, or DUPLICATE: FALSE.
"""


async def shoot_img_ds(jobs: list[tuple[str, str]]):
    """Function to process the entire dataset (asynchronously) and get responses.

    Args:
        jobs (list[tuple[str, str]]): list of tuples containing (instructions and image paths).

    Returns:
        dict: mapping of image path to list of TRUE / FALSE (model predictions).
    """
    results = {}
    sem = asyncio.Semaphore(MAX_INFLIGHT)  # will control that no more than MAX_INFLIGHT requests go to the vllm-server
    pbar = tqdm(total=len(jobs), desc="Requests done", unit="request")

    async with AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL,) as client:  # timeout=0)
        async def runner(idx, instr, path, seed4generation):
            try:
                async with sem:
                    # send a request in which N_PER_REQUEST sequences will be generated
                    res = await one_call(client, instr, path, seed=seed4generation)
                results[path] = res  # [answer1, ..., answerN]
            except Exception as e:
                print('Exception with path:', path)
                print(str(e))
            finally:
                await dump_sample(path, instr, results[path])
                pbar.update(1)

        # launch tasks, put them in the queue for execution
        tasks = [asyncio.create_task(runner(i, instr, path, seed)) for i, (instr, path, seed) in enumerate(jobs)]
        await asyncio.gather(*tasks)  # wait for all to complete

    pbar.close()
    return results


async def dump_sample(path: str, instruction: str, info: str):
    """Function to save model predictions to a file.

    Args:
        path (str): Path to the image.
        instruction (str): Input prompt for the model.
        info (str): Model responses after postprocessing.

    Returns:
        None
    """
    async with aiofiles.open(DUMP_INTO, "a") as file:
        await file.write(json.dumps({'image_path': path, 'prompt': instruction, 'answers': info}) + "\n")
        await file.flush()


if __name__ == "__main__":
    df = pd.read_csv("../annotations.tsv", sep="\t")
    basepath = "path_to_generated_images"
    models = os.listdir(basepath) 
    homonyms = df['Слово/фраза (eng)'].unique().tolist()

    strategy = input("Enter the strategy for forming meanings (prompt1 / prompt2 / prompt3): ")
    if strategy in MEANING_STRATEGIES.keys():
        print(f"{strategy} selected.")
        builder4meanings = MEANING_STRATEGIES[strategy]
    else:
        raise("Incorrect strategy name.")

    global DUMP_INTO
    DUMP_INTO = os.path.join("history", f"{strategy}_results.jsonl")
    if not os.path.exists(os.path.dirname(DUMP_INTO)):
        os.makedirs(os.path.dirname(DUMP_INTO))

    jobs = []
    instructions = []

    # form a dataset for processing (by homonyms, models and saved photos)
    for homonymword in homonyms[:]:
        subset = df[df['Слово/фраза (eng)'] == homonymword].copy()
        senses_as_string = builder4meanings(subset)  # collected all ru-sense(optionally) + ru-value/eng-value
        instruction = template(homonymword, senses_as_string)
        instructions.append(instruction)

        for modelname in models[:]:
            for seed in range(50):
                imgpath = os.path.join(basepath, modelname, homonymword, f"{seed}.png")
                jobs.append((instruction, imgpath, seed))

    path2answers = asyncio.run(shoot_img_ds(jobs=jobs))
